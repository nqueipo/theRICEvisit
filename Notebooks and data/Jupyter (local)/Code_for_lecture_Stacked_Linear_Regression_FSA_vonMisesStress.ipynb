{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0S3_3duDVfQW"
   },
   "source": [
    "<img src=\"https://www.rice.edu/_images/rice-logo.jpg\" width=\"200\">\n",
    "\n",
    "# Ensemble Learning - Stacking Linear Regression (SLR)\n",
    "## The RICE visit - June 17th, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Kv2QdBYVfQm"
   },
   "source": [
    "### Software dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T17:52:54.180262Z",
     "start_time": "2019-06-08T17:52:51.327094Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "QnHmnBI7VfQ1",
    "outputId": "8283ed43-829a-4ad6-b632-50237609f567"
   },
   "outputs": [],
   "source": [
    "#!pip install git+git://github.com/rasbt/mlxtend.git@v0.16.0\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "\n",
    "# MLXtend (http://rasbt.github.io/mlxtend/)\n",
    "from mlxtend.regressor import StackingRegressor, StackingCVRegressor\n",
    "\n",
    "# Scikit-learn (https://scikit-learn.org/stable/)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as Co\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JufR_hT0VfRS"
   },
   "source": [
    "### Illustrative example - Test function and design of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T17:52:54.191263Z",
     "start_time": "2019-06-08T17:52:54.183262Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SzG0hcqJVfRZ"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  \"\"\"\n",
    "  Data loading and sample splitting\n",
    "  \n",
    "  Output: dictionary with training (X, y) and testing (Xtest, ytest) data\n",
    "  \"\"\"\n",
    "  \n",
    "  # Loading sample dataset - FSAE Romero & Queipo 2017\n",
    "  print('Loading data...', end=' ')\n",
    "    \n",
    "  fsae_df = pd.read_csv('fsae_case_study.csv', delimiter=',', index_col=None, decimal='.')\n",
    "  (sample_size, dim) = fsae_df.shape\n",
    "  print(fsae_df.shape)\n",
    "   \n",
    "  # Natural Logarithm - Young modulus\n",
    "  fsae_df.iloc[:,-2] = np.log(fsae_df.iloc[:,-2])\n",
    "  \n",
    "  # Normalizing data\n",
    "  scaler = StandardScaler().fit(fsae_df.iloc[:,0:-1])\n",
    "  fsae_df.iloc[:,0:-1] = scaler.transform(fsae_df.iloc[:,0:-1])\n",
    "  \n",
    "  # Data splitting\n",
    "  idxs = random.sample(range(len(fsae_df)), k=180)\n",
    "  X = fsae_df.iloc[idxs,0:-1].values\n",
    "  y = fsae_df.iloc[idxs,-1].values\n",
    "  compl_idxs = [i for i in range(len(fsae_df)) if i not in idxs]\n",
    "  Xtest = fsae_df.iloc[compl_idxs,0:-1].values\n",
    "  ytest = fsae_df.iloc[compl_idxs,-1].values\n",
    "    \n",
    "  return { 'X':X, 'y':y, 'Xtest':Xtest, 'ytest':ytest }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3pyQnWmVfRq"
   },
   "source": [
    "### Grid search definition & model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T17:52:54.316270Z",
     "start_time": "2019-06-08T17:52:54.195263Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jmmd3_JDVfRv"
   },
   "outputs": [],
   "source": [
    "def model_train(seed, sample):\n",
    "  \"\"\"\n",
    "  Base learner and SLR definitions, hyper-parameters grid, training\n",
    "  \n",
    "  Input: sample - dictionary with training (X, y) data\n",
    "  Output: dictionary with trained base learners and SLR\n",
    "  \"\"\"\n",
    "  \n",
    "  # Stacking regressor object\n",
    "  # SVR: Support Vector Machine \n",
    "  # LinearRegression: Basic linear regression\n",
    "  # MLPRegressor: Neural Network \n",
    "  # GaussianProcessRegressor: Kriging model \n",
    "  slreg = StackingCVRegressor( \\\n",
    "    regressors=[ \\\n",
    "      SVR(kernel='rbf'), \\\n",
    "      Pipeline([('preprocessing',PolynomialFeatures(1)),('regressor',LinearRegression())]), \\\n",
    "      MLPRegressor(solver='lbfgs', random_state=seed), \\\n",
    "      GaussianProcessRegressor(alpha=0.5, n_restarts_optimizer=4, normalize_y=False, random_state=seed, \\\n",
    "        kernel=Co(40.0, (1e-3, 1e2))*RBF(length_scale=0.45, length_scale_bounds=(1e-2, 1e1))) \\\n",
    "      ], \\\n",
    "    meta_regressor=LinearRegression(fit_intercept=False), cv=5, random_state=seed, n_jobs=-1)\n",
    "  \n",
    "  (sample_size, dim) = sample['X'].shape\n",
    "  \n",
    "  # Hyper-parameters setting\n",
    "  # Cherkassky & Ma reference values: C = 117.278, e = 10.333\n",
    "  hypars_bl = {}\n",
    "  hypars_bl['svr_r']  = {'C': [60, 120, 240], 'epsilon': [0.1, 1, 10]}\n",
    "  hypars_bl['nn']     = {'hidden_layer_sizes':[math.floor(f*(sample_size-1)/(dim+2)) for f in [0.50, 0.75, 1.0]], 'activation':['relu','logistic'], }\n",
    "  \n",
    "  # Grid building\n",
    "  hypars_slr = { \\\n",
    "    'svr__C':       hypars_bl['svr_r']['C'], \\\n",
    "    'svr__epsilon': hypars_bl['svr_r']['epsilon'], \\\n",
    "    'mlpregressor__hidden_layer_sizes':hypars_bl['nn']['hidden_layer_sizes'], \\\n",
    "    'mlpregressor__activation':        hypars_bl['nn']['activation'] }\n",
    "  slr_grid = GridSearchCV(estimator=slreg, param_grid=hypars_slr, cv=5, refit=True, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "  \n",
    "  # Models list\n",
    "  model_labels = ['SVR-RBF', 'LR','MLP','GP','SLR']\n",
    "  models = [GridSearchCV(estimator=SVR(kernel='rbf'), param_grid=hypars_bl['svr_r'], cv=5, refit=True, scoring='neg_mean_squared_error'), \\\n",
    "          Pipeline([('preprocessing',PolynomialFeatures(1)),('regressor',LinearRegression())]), \\\n",
    "          GridSearchCV(estimator=MLPRegressor(solver='lbfgs', random_state=seed), param_grid=hypars_bl['nn'], cv=5, refit=True, scoring='neg_mean_squared_error'), \\\n",
    "          GaussianProcessRegressor(alpha=0.5, n_restarts_optimizer=4, normalize_y=False, random_state=seed, \\\n",
    "                                   kernel=Co(40.0, (1e-3, 1e2))*RBF(length_scale=0.45, length_scale_bounds=(1e-2, 1e1))),\n",
    "          slr_grid\n",
    "         ]\n",
    "\n",
    "  # Training models\n",
    "  for model in models:\n",
    "    print('{}: '.format(model.__class__.__name__), end='')\n",
    "    tmp = time.time()    \n",
    "    # call fit function using python's getattr function\n",
    "    getattr(model, 'fit')(sample['X'], sample['y'])\n",
    "    print(' {:0.3}s'.format(time.time()-tmp)) #time.strftime(\"%M:%S.%f\", time.gmtime(time.time()-tmp))))\n",
    "      \n",
    "  return { 'models':models, 'model_labels':model_labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T17:52:54.399274Z",
     "start_time": "2019-06-08T17:52:54.318270Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "T_X4teltVfR_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sample, models_bundle):\n",
    "  \"\"\"\n",
    "  Models evaluation\n",
    "  \n",
    "  Input: sample - dictionary with training (X, y) and testing (Xtest, ytest) data\n",
    "         models_bundle - dictionary with trained base learners and SLR\n",
    "  Output: dictionary with performance indicators (indicators) and SLR \n",
    "          meta-regressor coefficients (coefficients)\n",
    "  \"\"\"\n",
    "  \n",
    "  models = models_bundle['models']\n",
    "  model_labels = models_bundle['model_labels']\n",
    "\n",
    "  #Performance indicators (individuals, SLR)\n",
    "  answer = { 'Method':[ label for label in model_labels ]}\n",
    "  # Coefficient of determination\n",
    "  answer['R2']    = [ r2_score(sample['y'], mod.predict(sample['X'])) for mod in models ]\n",
    "  # Mean squared error\n",
    "  answer['MSEtr'] = [ mean_squared_error(sample['y'], mod.predict(sample['X'])) for mod in models ]\n",
    "  answer['MSEte'] = [ mean_squared_error(sample['ytest'], mod.predict(sample['Xtest'])) for mod in models ]\n",
    "  # Median absolute error\n",
    "  answer['MAEtr'] = [ median_absolute_error(sample['y'], mod.predict(sample['X'])) for mod in models ]\n",
    "  answer['MAEte'] = [ median_absolute_error(sample['ytest'], mod.predict(sample['Xtest'])) for mod in models ]\n",
    "  indicators = pd.DataFrame(answer)\n",
    "  \n",
    "  #Errors (Committee)\n",
    "  indicators = indicators.append({ 'Method': 'Committee', \\\n",
    "      # Coefficient of determination\n",
    "      'R2': r2_score(sample['y'], np.average(list(map(lambda x: x.predict(sample['X']), models[0:-1])), axis=0)), \\\n",
    "      # Mean squared error\n",
    "      'MSEtr': mean_squared_error(sample['y'], np.average(list(map(lambda x: x.predict(sample['X']), models[0:-1])), axis=0)), \\\n",
    "      'MSEte': mean_squared_error(sample['ytest'], np.average(list(map(lambda x: x.predict(sample['Xtest']), models[0:-1])), axis=0)), \\\n",
    "      # Median absolute error\n",
    "      'MAEtr': median_absolute_error(sample['y'], np.average(list(map(lambda x: x.predict(sample['X']), models[0:-1])), axis=0)), \\\n",
    "      'MAEte': median_absolute_error(sample['ytest'], np.average(list(map(lambda x: x.predict(sample['Xtest']), models[0:-1])), axis=0)), \\\n",
    "    }, ignore_index=True)\n",
    "  \n",
    "  print(\"%25s\\t%10s\\t%10s\\t%10s\" % (\"Ensemble\", \"Score (R2)\", \"MSE(train)\", \"MSE(test)\"))\n",
    "  for index, row in indicators.iterrows():\n",
    "    print(\"%25s\\t%10.3f\\t%10.3f\\t%10.3f\" % (row['Method'], row['R2'], row['MSEtr'], row['MSEte']))  \n",
    "\n",
    "  print(\"\\nParameters of interest\")\n",
    "  print(\"%25s\\t%s\" % (\"SVR-RBF\", models[0].best_params_)) \n",
    "  print(\"%25s\\t%s\" % (\"LR\", \"Coefs:  [1, x1, x2, x1^2, x1*x2, x2^2] - %s\"%(models[1].named_steps['regressor'].coef_)))\n",
    "  print(\"%25s\\t%s\" % (\"MLP\", models[2].best_params_)) \n",
    "  print(\"%25s\\t%s\" % (\"GP\", \"Kern: %s - alpha: %s\"%(models[3].kernel_, np.mean(models[3].alpha_))))\n",
    "  print(\"%25s\\t%s\" % (\"SLR\", \"Coefs: %s - %s\"%([i for i in models[4].best_estimator_.named_regressors], models[4].best_estimator_.meta_regr_.coef_)))\n",
    "    \n",
    "  return { 'indicators': indicators, 'coefficients': models[4].best_estimator_.meta_regr_.coef_ }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwvWEvs7U0ru"
   },
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-08T17:52:51.347Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12430
    },
    "colab_type": "code",
    "id": "cS2AHrdNVfSQ",
    "outputId": "443b2c67-c64a-47f7-910c-bb016d31c039",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files on disk: 55\n",
      "[00:00:00]Seed 1003 ready!\n",
      "[00:00:00]Seed 1019 ready!\n",
      "[00:00:00]Seed 1024 ready!\n",
      "[00:00:00]Seed 133 ready!\n",
      "[00:00:00]Seed 1396 ready!\n",
      "[00:00:00]Seed 1438 ready!\n",
      "[00:00:00]Seed 1445 ready!\n",
      "[00:00:00]Seed 1447 ready!\n",
      "[00:00:00]Seed 1498 ready!\n",
      "[00:00:00]Seed 1520 ready!\n",
      "[00:00:00]Seed 1534 ready!\n",
      "[00:00:00]Seed 1688 ready!\n",
      "[00:00:00]Seed 1705 ready!\n",
      "[00:00:00]Seed 1712 ready!\n",
      "[00:00:00]Seed 1768 ready!\n",
      "[00:00:00]Seed 1848 ready!\n",
      "[00:00:00]Seed 1953 ready!\n",
      "[00:00:00]Seed 2050 ready!\n",
      "[00:00:00]Seed 2108 ready!\n",
      "[00:00:00]Seed 2148 ready!\n",
      "[00:00:00]Seed 2161 ready!\n",
      "[00:00:00]Seed 2231 ready!\n",
      "[00:00:00]Seed 2232 ready!\n",
      "[00:00:00]Seed 2248 ready!\n",
      "[00:00:00]Seed 2306 ready!\n",
      "[00:00:00]Seed 2389 ready!\n",
      "[00:00:00]Seed 2489 ready!\n",
      "[00:00:00]Seed 2492 ready!\n",
      "[00:00:00]Seed 2524 ready!\n",
      "[00:00:00]Seed 2592 ready!\n",
      "[00:00:00]Seed 2707 ready!\n",
      "[00:00:00]Seed 2710 ready!\n",
      "[00:00:00]Seed 2779 ready!\n",
      "[00:00:00]Seed 3044 ready!\n",
      "[00:00:00]Seed 3098 ready!\n",
      "[00:00:00]Seed 32 ready!\n",
      "[00:00:00]Seed 3284 ready!\n",
      "[00:00:00]Seed 3329 ready!\n",
      "[00:00:00]Seed 3361 ready!\n",
      "[00:00:00]Seed 3372 ready!\n",
      "[00:00:00]Seed 3576 ready!\n",
      "[00:00:00]Seed 3603 ready!\n",
      "[00:00:00]Seed 3610 ready!\n",
      "[00:00:00]Seed 363 ready!\n",
      "[00:00:00]Seed 3632 ready!\n",
      "[00:00:00]Seed 372 ready!\n",
      "[00:00:00]Seed 3742 ready!\n",
      "[00:00:00]Seed 3744 ready!\n",
      "[00:00:00]Seed 383 ready!\n",
      "[00:00:00]Seed 3935 ready!\n",
      "[00:00:00]Loading data... \n",
      "Loading data... (1320, 10)\n",
      "[00:00:00]Training models...\n",
      "GridSearchCV:  0.251s\n",
      "Pipeline:  0.0355s\n",
      "GridSearchCV:  2.65s\n",
      "GaussianProcessRegressor:  0.899s\n",
      "GridSearchCV:  1.16e+02s\n",
      "[00:01:59]Calculating performance measures...\n",
      "                 Ensemble\tScore (R2)\tMSE(train)\t MSE(test)\n",
      "                  SVR-RBF\t     0.956\t    71.872\t   454.167\n",
      "                       LR\t     0.793\t   338.141\t   446.374\n",
      "                      MLP\t     0.964\t    59.375\t   397.847\n",
      "                       GP\t     1.000\t     0.210\t   709.605\n",
      "                      SLR\t     0.954\t    75.264\t   311.333\n",
      "                Committee\t     0.969\t    51.189\t   358.279\n",
      "\n",
      "Parameters of interest\n",
      "                  SVR-RBF\t{'C': 240, 'epsilon': 10}\n",
      "                       LR\tCoefs:  [1, x1, x2, x1^2, x1*x2, x2^2] - [  0.          -9.63186145   4.2607917   -4.21364719  -3.14654664\n",
      "  -3.02937764   6.7443976    4.1091968  -33.48173869  -0.31630819]\n",
      "                      MLP\t{'activation': 'relu', 'hidden_layer_sizes': 12}\n",
      "                       GP\tKern: 10**2 * RBF(length_scale=1.98) - alpha: 0.118173946631739\n",
      "                      SLR\tCoefs: [('svr', SVR(C=240, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "    tol=0.001, verbose=False)), ('pipeline', Pipeline(memory=None,\n",
      "         steps=[('preprocessing',\n",
      "                 PolynomialFeatures(degree=1, include_bias=True,\n",
      "                                    interaction_only=False, order='C')),\n",
      "                ('regressor',\n",
      "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "                                  normalize=False))],\n",
      "         verbose=False)), ('mlpregressor', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=12, learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=4019, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)), ('gaussianprocessregressor', GaussianProcessRegressor(alpha=0.5, copy_X_train=True,\n",
      "                         kernel=6.32**2 * RBF(length_scale=0.45),\n",
      "                         n_restarts_optimizer=4, normalize_y=False,\n",
      "                         optimizer='fmin_l_bfgs_b', random_state=4019))] - [ 0.30977725  0.38239029  0.35540848 -0.04728386]\n",
      "[00:01:59]Seed 4019 ready!51 of 150 total seeds.\n",
      "[00:01:59]Dumping to local disk file: fsae_von_180_curseed_4019.pkl ...\n",
      "DONE!\n",
      "[00:01:59]Loading data... \n",
      "Loading data... (1320, 10)\n",
      "[00:01:59]Training models...\n",
      "GridSearchCV:  0.235s\n",
      "Pipeline:  0.001s\n",
      "GridSearchCV:  2.35s\n",
      "GaussianProcessRegressor:  0.507s\n",
      "GridSearchCV:  1.57e+02s\n",
      "[00:04:40]Calculating performance measures...\n",
      "                 Ensemble\tScore (R2)\tMSE(train)\t MSE(test)\n",
      "                  SVR-RBF\t     0.983\t    40.519\t   438.383\n",
      "                       LR\t     0.783\t   514.560\t   436.072\n",
      "                      MLP\t     0.961\t    92.952\t   484.645\n",
      "                       GP\t     1.000\t     0.266\t   940.448\n",
      "                      SLR\t     0.939\t   144.881\t   329.544\n",
      "                Committee\t     0.972\t    65.929\t   350.277\n",
      "\n",
      "Parameters of interest\n",
      "                  SVR-RBF\t{'C': 240, 'epsilon': 1}\n",
      "                       LR\tCoefs:  [1, x1, x2, x1^2, x1*x2, x2^2] - [  0.         -10.7920627    5.94191382  -6.30152062  -3.22081121\n",
      "  -5.5937865    7.15524904   6.77840664 -37.84968652  -2.60122801]\n",
      "                      MLP\t{'activation': 'relu', 'hidden_layer_sizes': 8}\n",
      "                       GP\tKern: 10**2 * RBF(length_scale=1.8) - alpha: 0.1528663893963689\n",
      "                      SLR\tCoefs: [('svr', SVR(C=120, cache_size=200, coef0=0.0, degree=3, epsilon=1,\n",
      "    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
      "    tol=0.001, verbose=False)), ('pipeline', Pipeline(memory=None,\n",
      "         steps=[('preprocessing',\n",
      "                 PolynomialFeatures(degree=1, include_bias=True,\n",
      "                                    interaction_only=False, order='C')),\n",
      "                ('regressor',\n",
      "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "                                  normalize=False))],\n",
      "         verbose=False)), ('mlpregressor', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=16, learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=4237, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)), ('gaussianprocessregressor', GaussianProcessRegressor(alpha=0.5, copy_X_train=True,\n",
      "                         kernel=6.32**2 * RBF(length_scale=0.45),\n",
      "                         n_restarts_optimizer=4, normalize_y=False,\n",
      "                         optimizer='fmin_l_bfgs_b', random_state=4237))] - [ 0.27122545  0.40468083  0.36580439 -0.03741502]\n",
      "[00:04:40]Seed 4237 ready!52 of 150 total seeds.\n",
      "[00:04:40]Dumping to local disk file: fsae_von_180_curseed_4237.pkl ...\n",
      "DONE!\n",
      "[00:04:40]Loading data... \n",
      "Loading data... (1320, 10)\n",
      "[00:04:40]Training models...\n",
      "GridSearchCV:  0.262s\n",
      "Pipeline:  0.001s\n",
      "GridSearchCV:  3.17s\n",
      "GaussianProcessRegressor:  0.286s\n",
      "GridSearchCV: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define, train and evaluate models, and store performance measures in google drive\n",
    "\"\"\"\n",
    "    \n",
    "boots = 100\n",
    "\n",
    "# Predefined seeds (seeds = np.random.randint(10000, size=boots))\n",
    "seeds = np.asarray([1003, 1019, 1024, 133, 1396, 1438, 1445, 1447, 1498, 1520, 1534, 1688, \n",
    "                    1705, 1712, 1768, 1848, 1953, 2050, 2108, 2148, 2161, 2231, 2232, 2248, \n",
    "                    2306, 2389, 2489, 2492, 2524, 2592, 2707, 2710, 2779, 3044, 3098, 32, \n",
    "                    3284, 3329, 3361, 3372, 3576, 3603, 3610, 363, 3632, 372, 3742, 3744, \n",
    "                    383, 3935, 4019, 4237, 4272, 4363, 4464, 4604, 4718, 4721, 4834, 484, \n",
    "                    4884, 489, 4891, 4926, 4932, 5035, 5066, 5106, 5276, 545, 5525, 5660, \n",
    "                    5686, 5703, 5784, 5813, 5838, 5925, 5994, 6081, 6180, 6316, 6400, 6429, \n",
    "                    6523, 6542, 6557, 6655, 6664, 6688, 684, 6972, 7016, 7061, 7161, 7170, \n",
    "                    7188, 7190, 7201, 7309, 7328, 7341, 7391, 754, 7548, 7713, 7749, 7756, \n",
    "                    7784, 7818, 7895, 8011, 8021, 8055, 8060, 8073, 8106, 8127, 8145, 8374, \n",
    "                    8434, 8466, 8487, 8536, 858, 860, 8675, 8689, 8839, 8849, 8882, 8896, \n",
    "                    8981, 9026, 9056, 9160, 9225, 9278, 9306, 9364, 9394, 9468, 9711, 9820, \n",
    "                    9834, 9859, 9936, 9981, 2403, 1904]) \n",
    "\n",
    "# Profiling\n",
    "start_time = time.time()\n",
    "\n",
    "# Auto-iterate through all files in the root folder.\n",
    "file_list = [file for file in os.listdir('.') if os.path.isfile(file)]\n",
    "print('Files on disk: {}'.format(len(file_list)))\n",
    "   \n",
    "# Main cycle\n",
    "for cur_seed in seeds[0:boots]:\n",
    "  filename = 'fsae_von_180_curseed_{}.pkl'.format(cur_seed)\n",
    "  matching_file = [ match for match in file_list if match == filename ]  # Get already simulated seeds\n",
    "  if len(matching_file) > 0:\n",
    "    # Seed already simulated. Skip to next cycle.\n",
    "    print('[%s]Seed %d ready!'% (time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),cur_seed)) \n",
    "    continue\n",
    "  \n",
    "  np.random.seed(cur_seed)\n",
    "  random.seed(cur_seed)\n",
    "  \n",
    "  # Loading train and test sets\n",
    "  print(\"[{}]Loading data... \".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "  sample = load_data()\n",
    "  \n",
    "  # Train individual models and SLR\n",
    "  print(\"[{}]Training models...\".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "  models_bundle = model_train(cur_seed, sample)\n",
    "  \n",
    "  # Calculate and print performance measures\n",
    "  print(\"[{}]Calculating performance measures...\".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "  evaluation = evaluate(sample, models_bundle)\n",
    "  \n",
    "  print('[%s]Seed %d ready!'% (time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),cur_seed), end='')\n",
    "  print('%d of %d total seeds.' % (seeds.tolist().index(cur_seed)+1, len(seeds)))\n",
    "  \n",
    "  data = {'seed':cur_seed, 'sample':sample, 'models_bundle':models_bundle, 'evaluation':evaluation }\n",
    "  \n",
    "  # Dumping state to disk...\n",
    "  print('[{}]Dumping to local disk file: {} ...'.format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),filename))\n",
    "  with open(filename, 'wb') as f:  \n",
    "    pickle.dump([data], f)\n",
    "  \n",
    "  print('DONE!')\n",
    "  continue\n",
    "\n",
    "print(\"DONE!\")\n",
    "print (\"All seeds evaluated!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnzQIkvEVfSx"
   },
   "source": [
    "### Visualization (individual model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-08T17:52:51.350Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "079N15oOVfS2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization of results\n",
    "\"\"\"\n",
    "\n",
    "# Google drive: Authenticate and create the PyDrive client\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Google drive: Reading available files on disk\n",
    "file_list = [file for file in os.listdir('.') if os.path.isfile(file)]\n",
    "print('Files on disk: {}'.format(len(file_list)))\n",
    "\n",
    "# PICKLE files\n",
    "file_pattern = 'fsae_von_180_'\n",
    "\n",
    "file_targets = [ match for match in file_list if match.startswith(file_pattern) ]  # Get already simulated seeds\n",
    "print('Target files: {}'.format(len(file_targets)))\n",
    "unified_data = pd.DataFrame()\n",
    "unified_coef = []\n",
    "\n",
    "win_freq = []\n",
    "names = ['SVR-RBF','LR','MLP','GP','Committee','SLR']\n",
    "for file in file_targets: \n",
    "  with open(file['title'],'rb') as f:\n",
    "    data = pickle.load(f)[0]\n",
    "  data['evaluation'] = evaluate(data['sample'],data['models_bundle'])  \n",
    "  evaluation = data['evaluation']\n",
    "  unified_data = unified_data.append(data['evaluation']['indicators']) \n",
    "  unified_coef.append(data['evaluation']['coefficients'])\n",
    "  unified_data = unified_data.append(data['evaluation']['indicators']) \n",
    "  # Performance measures unification\n",
    "  # Coefficient of determination\n",
    "  r2s = [ r2_score(data['sample']['y'], cur.predict(data['sample']['X'])) for cur in data['models_bundle']['models'] ]\n",
    "  r2s[4:4] = [r2_score(data['sample']['y'], np.average(list(map(lambda x: x.predict(data['sample']['X']), data['models_bundle']['models'][0:-1])), axis=0))]\n",
    "  # Mean squared error\n",
    "  mses = [ mean_squared_error(data['sample']['ytest'], cur.predict(data['sample']['Xtest'])) for cur in data['models_bundle']['models'] ]\n",
    "  mses[4:4] = [mean_squared_error(data['sample']['ytest'], np.average(list(map(lambda x: x.predict(data['sample']['Xtest']), data['models_bundle']['models'][0:-1])), axis=0))]\n",
    "  # Median absolute error\n",
    "  maes = [ median_absolute_error(data['sample']['ytest'], cur.predict(data['sample']['Xtest'])) for cur in data['models_bundle']['models'] ]\n",
    "  maes[4:4] = [median_absolute_error(data['sample']['ytest'], np.average(list(map(lambda x: x.predict(data['sample']['Xtest']), data['models_bundle']['models'][0:-1])), axis=0))]\n",
    "  # Winner model (smaller MSE) identification\n",
    "  win_idx = np.where(mses == np.amin(mses))\n",
    "  win_freq.append(names[win_idx[0][0]])\n",
    "  # Specific seed visualization\n",
    "  if data['seed'] == 1024:\n",
    "    print('Weights for seed {}: {}'.format(data['seed'], evaluation['coefficients']))    \n",
    "    print(evaluation['indicators'])\n",
    "        \n",
    "# Set (te | tr)\n",
    "theSet = \"te\"\n",
    "# The indicator (MSE | MAE)\n",
    "theIndic = \"MSE\"\n",
    "# Indicator limit (sharing y-scales in continuous boxplots)\n",
    "maxY = 20    # MAE Testing\n",
    "# maxY = 2500  # MSE Testing\n",
    "# maxY = 700   # MSE Training\n",
    "\n",
    "toPlot = theIndic+theSet\n",
    "print('To plot: {}'.format(toPlot))\n",
    "\n",
    "tmp_palette = sns.color_palette(n_colors=5)\n",
    "tmp_palette.append((0.3, 0.6, 0.9))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10), gridspec_kw={'width_ratios': [4, 1, 1]})#, 'height_ratios':[3, 1.5]})\n",
    "sns.boxplot(data=unified_data.query('Method in [\"SVR-RBF\",\"LR\",\"MLP\",\"GP\"]'), x='Method', y=toPlot, notch=True, ax=axes[0][0], width=0.5, palette=tmp_palette)\n",
    "axes[0][0].set_ylabel('MSE')\n",
    "axes[0][0].set_xlabel(' ')\n",
    "axes[0][0].set_title('Individual surrogates')\n",
    "axes[0][0].set_ylim(0, maxY)#SLR-CV\n",
    "sns.boxplot(data=unified_data.query('Method == \"Committee\"'), x='Method', y=toPlot, notch=True, ax=axes[0][1], color=sns.color_palette(n_colors=5)[-1], width=0.5)\n",
    "axes[0][1].set_ylabel('MSE')\n",
    "axes[0][1].set_xlabel(' ')\n",
    "axes[0][1].set_title(\"Committee\")\n",
    "axes[0][1].set_ylim(0, maxY)#SLR-CV\n",
    "axes[0][1].axes.get_yaxis().set_ticklabels([])\n",
    "axes[0][1].axes.get_xaxis().set_ticklabels([])\n",
    "sns.boxplot(data=unified_data.query('Method == \"SLR\"'), x='Method', y=toPlot, notch=True, ax=axes[0][2], color=(0.3, 0.6, 0.9), width=0.5)\n",
    "axes[0][2].set_ylabel('MSE')\n",
    "axes[0][2].set_xlabel(' ')\n",
    "axes[0][2].set_title('SLR')\n",
    "axes[0][2].set_ylim(0, maxY)#SLR-CV\n",
    "axes[0][2].axes.get_yaxis().set_ticklabels([])\n",
    "axes[0][2].axes.get_xaxis().set_ticklabels([])\n",
    "\n",
    "sns.boxplot(x=\"variable\",y=\"value\",data=pd.melt(pd.DataFrame(unified_coef, columns=['SVR','LR ','MLP','GP'])), notch=True, width=0.4, ax=axes[1][0], palette=tmp_palette)\n",
    "axes[1][0].set_xlabel('Meta-regressor coefficients')\n",
    "axes[1][0].set_ylabel('Coefficient value')\n",
    "axes[1][0].axes.get_xaxis().set_ticklabels([])\n",
    "\n",
    "axes[1][0].set_xlabel('Meta-regressor coefficients')\n",
    "\n",
    "f, ax1 = plt.subplots(figsize=(5.3, 3))\n",
    "freqs = [ win_freq.count(X) for X in names]\n",
    "sns.barplot(data=pd.DataFrame({'models':names,'Frequency':freqs}), x='models', y='Frequency', palette=tmp_palette)\n",
    "\n",
    "print(\"\\nMSE\\tTrain\\tTest\\nCommittee\\t{:9.3f}\\t{:9.3f}\\nSLR\\t{:9.3f}\\t{:9.3f}\".format( \\\n",
    "   np.median(unified_data[unified_data.Method == 'Committee']['MSEtr']), \\\n",
    "   np.median(unified_data[unified_data.Method == 'Committee']['MSEte']), \\\n",
    "   np.median(unified_data[unified_data.Method == 'SLR']['MSEtr']), \\\n",
    "   np.median(unified_data[unified_data.Method == 'SLR']['MSEte'])))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Code for lecture - Stacked Linear Regression - FSA-vonMisesStress.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
