{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0S3_3duDVfQW"
   },
   "source": [
    "<img src=\"https://www.rice.edu/_images/rice-logo.jpg\" width=\"200\">\n",
    "   \n",
    "# Ensemble Learning - Stacking Linear Regression (SLR)\n",
    "## The RICE visit - June 17th, 2019 - Nestor V. Queipo\n",
    "<br/><a href=\"https://colab.research.google.com/github/nqueipo/theRICEvisit/blob/master/Notebook%20and%20data/Code_for_lecture-Stacked_Linear_Regression.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "<p style=\"text-align: center\">(To open the link from Github, please keep the <kbd>Ctrl</kbd> key pressed while clicking the \"Open in Colab\" badge)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Kv2QdBYVfQm"
   },
   "source": [
    "### Software dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T12:51:41.115071Z",
     "start_time": "2019-06-13T12:51:19.375859Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "QnHmnBI7VfQ1",
    "outputId": "8283ed43-829a-4ad6-b632-50237609f567"
   },
   "outputs": [],
   "source": [
    "!pip install git+git://github.com/rasbt/mlxtend.git@v0.16.0\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "# MLXtend (http://rasbt.github.io/mlxtend/)\n",
    "from mlxtend.regressor import StackingRegressor, StackingCVRegressor\n",
    "# Scikit-learn (https://scikit-learn.org/stable/)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as Co\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggested versions: <pre>\n",
    "numpy: 1.16.4\n",
    "pandas: 0.24.2\n",
    "sklearn: 0.21.2\n",
    "mlxtend: 0.16.0\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T12:51:41.127071Z",
     "start_time": "2019-06-13T12:51:41.117071Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Used versions:\\n\\n    numpy: {}'.format(np.__version__))\n",
    "print('    pandas: {}'.format(pd.__version__))\n",
    "import sklearn as skl\n",
    "print('    sklearn: {}'.format(skl.__version__))\n",
    "import mlxtend as mlx\n",
    "print('    mlxtend: {}'.format(mlx.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JufR_hT0VfRS"
   },
   "source": [
    "### FSAE case study data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T12:51:41.231077Z",
     "start_time": "2019-06-13T12:51:41.129072Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SzG0hcqJVfRZ"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  \"\"\"\n",
    "  Data loading and sample splitting\n",
    "  \n",
    "  Output: dictionary with training (X, y) and testing (Xtest, ytest) data\n",
    "  \"\"\"\n",
    "  \n",
    "  # Loading sample dataset - FSAE Romero & Queipo 2017\n",
    "  print('Loading data...', end=' ')\n",
    "    \n",
    "  # Download data from Github to current instance\n",
    "  data_url = 'https://raw.githubusercontent.com/nqueipo/theRICEvisit/master/Notebook%20and%20data/fsae_case_study.csv'  \n",
    "  fsae_df = pd.read_csv(data_url, delimiter=',', index_col=None, decimal='.')\n",
    "  (sample_size, dim) = fsae_df.shape\n",
    "  print(fsae_df.shape)\n",
    "   \n",
    "  # Natural Logarithm - Young modulus\n",
    "  fsae_df.iloc[:,-2] = np.log(fsae_df.iloc[:,-2])\n",
    "  \n",
    "  # Normalizing data\n",
    "  scaler = StandardScaler().fit(fsae_df.iloc[:,0:-1])\n",
    "  fsae_df.iloc[:,0:-1] = scaler.transform(fsae_df.iloc[:,0:-1])\n",
    "  \n",
    "  # Data splitting\n",
    "  idxs = random.sample(range(len(fsae_df)), k=90)\n",
    "  X = fsae_df.iloc[idxs,0:-1].values\n",
    "  y = fsae_df.iloc[idxs,-1].values\n",
    "  compl_idxs = [i for i in range(len(fsae_df)) if i not in idxs]\n",
    "  Xtest = fsae_df.iloc[compl_idxs,0:-1].values\n",
    "  ytest = fsae_df.iloc[compl_idxs,-1].values\n",
    "    \n",
    "  return { 'X':X, 'y':y, 'Xtest':Xtest, 'ytest':ytest }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3pyQnWmVfRq"
   },
   "source": [
    "### Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T12:51:41.316082Z",
     "start_time": "2019-06-13T12:51:41.232078Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jmmd3_JDVfRv"
   },
   "outputs": [],
   "source": [
    "def model_train(seed, sample):\n",
    "  \"\"\"\n",
    "  Base learner and SLR definitions, hyper-parameters grid, training\n",
    "  \n",
    "  Input: sample - dictionary with training (X, y) data\n",
    "  Output: dictionary with trained base learners and SLR\n",
    "  \"\"\"\n",
    "  \n",
    "  # Stacking regressor object\n",
    "  # SVR: Support Vector Machine \n",
    "  # LinearRegression: Basic linear regression\n",
    "  # MLPRegressor: Neural Network \n",
    "  # GaussianProcessRegressor: Kriging model \n",
    "  slreg = StackingCVRegressor( \\\n",
    "    regressors=[ \\\n",
    "      SVR(kernel='rbf'), \\\n",
    "      Pipeline([('preprocessing',PolynomialFeatures(1)),('regressor',LinearRegression())]), \\\n",
    "      MLPRegressor(solver='lbfgs', random_state=seed), \\\n",
    "      GaussianProcessRegressor(alpha=0.5, n_restarts_optimizer=4, normalize_y=False, random_state=seed, \\\n",
    "        kernel=Co(40.0, (1e-3, 1e2))*RBF(length_scale=0.45, length_scale_bounds=(1e-2, 1e1))) \\\n",
    "      ], \\\n",
    "    meta_regressor=LinearRegression(fit_intercept=False), cv=5, random_state=seed, n_jobs=-1)\n",
    "  \n",
    "  (sample_size, dim) = sample['X'].shape\n",
    "  \n",
    "  # Hyper-parameters setting\n",
    "  # Cherkassky & Ma reference values: C = 117.278, e = 10.333\n",
    "  hypars_bl = {}\n",
    "  hypars_bl['svr_r']  = {'C': [60, 120, 240], 'epsilon': [0.1, 1, 10]}\n",
    "  hypars_bl['nn']     = {'hidden_layer_sizes':[math.floor(f*(sample_size-1)/(dim+2)) for f in [0.50, 0.75, 1.0]], 'activation':['relu','logistic'], }\n",
    "  \n",
    "  # Grid building\n",
    "  hypars_slr = { \\\n",
    "    'svr__C':       hypars_bl['svr_r']['C'], \\\n",
    "    'svr__epsilon': hypars_bl['svr_r']['epsilon'], \\\n",
    "    'mlpregressor__hidden_layer_sizes':hypars_bl['nn']['hidden_layer_sizes'], \\\n",
    "    'mlpregressor__activation':        hypars_bl['nn']['activation'] }\n",
    "  slr_grid = GridSearchCV(estimator=slreg, param_grid=hypars_slr, cv=5, refit=True, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "  \n",
    "  # Models list\n",
    "  model_labels = ['SVR-RBF', 'LR','MLP','GP','SLR']\n",
    "  models = [GridSearchCV(estimator=SVR(kernel='rbf'), param_grid=hypars_bl['svr_r'], cv=5, refit=True, scoring='neg_mean_squared_error'), \\\n",
    "          Pipeline([('preprocessing',PolynomialFeatures(1)),('regressor',LinearRegression())]), \\\n",
    "          GridSearchCV(estimator=MLPRegressor(solver='lbfgs', random_state=seed), param_grid=hypars_bl['nn'], cv=5, refit=True, scoring='neg_mean_squared_error'), \\\n",
    "          GaussianProcessRegressor(alpha=0.5, n_restarts_optimizer=4, normalize_y=False, random_state=seed, \\\n",
    "                                   kernel=Co(40.0, (1e-3, 1e2))*RBF(length_scale=0.45, length_scale_bounds=(1e-2, 1e1))),\n",
    "          slr_grid\n",
    "         ]\n",
    "\n",
    "  # Training models\n",
    "  for idx in range(len(models)):\n",
    "    print('{}: '.format(model_labels[idx]), end='')\n",
    "    tmp = time.time()    \n",
    "    # call fit function using python's getattr function\n",
    "    getattr(models[idx], 'fit')(sample['X'], sample['y'])\n",
    "    print(' {:0.3}s'.format(time.time()-tmp)) #time.strftime(\"%M:%S.%f\", time.gmtime(time.time()-tmp))))\n",
    "      \n",
    "  return { 'models':models, 'model_labels':model_labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T12:51:41.414088Z",
     "start_time": "2019-06-13T12:51:41.318082Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "T_X4teltVfR_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sample, models_bundle):\n",
    "  \"\"\"\n",
    "  Models evaluation\n",
    "  \n",
    "  Input: sample - dictionary with training (X, y) and testing (Xtest, ytest) data\n",
    "         models_bundle - dictionary with trained base learners and SLR\n",
    "  Output: dictionary with performance indicators (indicators) and SLR \n",
    "          meta-regressor coefficients (coefficients)\n",
    "  \"\"\"\n",
    "  \n",
    "  models = models_bundle['models']\n",
    "  model_labels = models_bundle['model_labels']\n",
    "\n",
    "  #Performance indicators (individuals, SLR)\n",
    "  answer = { 'Method':[ label for label in model_labels ]}\n",
    "  # Coefficient of determination\n",
    "  answer['R2']    = [ r2_score(sample['y'], mod.predict(sample['X'])) for mod in models ]\n",
    "  # Mean squared error\n",
    "  answer['MSEtr'] = [ mean_squared_error(sample['y'], mod.predict(sample['X'])) for mod in models ]\n",
    "  answer['MSEte'] = [ mean_squared_error(sample['ytest'], mod.predict(sample['Xtest'])) for mod in models ]\n",
    "  # Median absolute error\n",
    "  answer['MAEtr'] = [ median_absolute_error(sample['y'], mod.predict(sample['X'])) for mod in models ]\n",
    "  answer['MAEte'] = [ median_absolute_error(sample['ytest'], mod.predict(sample['Xtest'])) for mod in models ]\n",
    "  indicators = pd.DataFrame(answer)\n",
    "  \n",
    "  #Errors (Committee)\n",
    "  indicators = indicators.append({ 'Method': 'Committee', \\\n",
    "      # Coefficient of determination\n",
    "      'R2': r2_score(sample['y'], np.average(list(map(lambda x: x.predict(sample['X']), models[0:-1])), axis=0)), \\\n",
    "      # Mean squared error\n",
    "      'MSEtr': mean_squared_error(sample['y'], np.average(list(map(lambda x: x.predict(sample['X']), models[0:-1])), axis=0)), \\\n",
    "      'MSEte': mean_squared_error(sample['ytest'], np.average(list(map(lambda x: x.predict(sample['Xtest']), models[0:-1])), axis=0)), \\\n",
    "      # Median absolute error\n",
    "      'MAEtr': median_absolute_error(sample['y'], np.average(list(map(lambda x: x.predict(sample['X']), models[0:-1])), axis=0)), \\\n",
    "      'MAEte': median_absolute_error(sample['ytest'], np.average(list(map(lambda x: x.predict(sample['Xtest']), models[0:-1])), axis=0)), \\\n",
    "    }, ignore_index=True)\n",
    "  \n",
    "  print(\"%25s\\t%10s\\t%10s\\t%10s\" % (\"Ensemble\", \"Score (R2)\", \"MSE(train)\", \"MSE(test)\"))\n",
    "  for index, row in indicators.iterrows():\n",
    "    print(\"%25s\\t%10.3f\\t%10.3f\\t%10.3f\" % (row['Method'], row['R2'], row['MSEtr'], row['MSEte']))  \n",
    "\n",
    "  print(\"\\nParameters of interest\")\n",
    "  print(\"%25s\\t%s\" % (\"SVR-RBF\", models[0].best_params_)) \n",
    "  print(\"%25s\\t%s\" % (\"LR\", \"Coefs:  [1, x1, x2, x1^2, x1*x2, x2^2] - %s\"%(models[1].named_steps['regressor'].coef_)))\n",
    "  print(\"%25s\\t%s\" % (\"MLP\", models[2].best_params_)) \n",
    "  print(\"%25s\\t%s\" % (\"GP\", \"Kern: %s - alpha: %s\"%(models[3].kernel_, np.mean(models[3].alpha_))))\n",
    "  print(\"%25s\\t%s\" % (\"SLR\", \"Coefs: %s - %s\"%([i for i in models[4].best_estimator_.named_regressors], models[4].best_estimator_.meta_regr_.coef_)))\n",
    "    \n",
    "  return { 'indicators': indicators, 'coefficients': models[4].best_estimator_.meta_regr_.coef_ }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwvWEvs7U0ru"
   },
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T12:53:02.648478Z",
     "start_time": "2019-06-13T12:51:41.416088Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12430
    },
    "colab_type": "code",
    "id": "cS2AHrdNVfSQ",
    "outputId": "443b2c67-c64a-47f7-910c-bb016d31c039",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define, train and evaluate models\n",
    "\"\"\"\n",
    "    \n",
    "boots = 150\n",
    "\n",
    "# Predefined seeds (seeds = np.random.randint(10000, size=boots))\n",
    "seeds = np.asarray([1003, 1019, 1024, 133, 1396, 1438, 1445, 1447, 1498, 1520, 1534, 1688, \n",
    "                    1705, 1712, 1768, 1848, 1953, 2050, 2108, 2148, 2161, 2231, 2232, 2248, \n",
    "                    2306, 2389, 2489, 2492, 2524, 2592, 2707, 2710, 2779, 3044, 3098, 32, \n",
    "                    3284, 3329, 3361, 3372, 3576, 3603, 3610, 363, 3632, 372, 3742, 3744, \n",
    "                    383, 3935, 4019, 4237, 4272, 4363, 4464, 4604, 4718, 4721, 4834, 484, \n",
    "                    4884, 489, 4891, 4926, 4932, 5035, 5066, 5106, 5276, 545, 5525, 5660, \n",
    "                    5686, 5703, 5784, 5813, 5838, 5925, 5994, 6081, 6180, 6316, 6400, 6429, \n",
    "                    6523, 6542, 6557, 6655, 6664, 6688, 684, 6972, 7016, 7061, 7161, 7170, \n",
    "                    7188, 7190, 7201, 7309, 7328, 7341, 7391, 754, 7548, 7713, 7749, 7756, \n",
    "                    7784, 7818, 7895, 8011, 8021, 8055, 8060, 8073, 8106, 8127, 8145, 8374, \n",
    "                    8434, 8466, 8487, 8536, 858, 860, 8675, 8689, 8839, 8849, 8882, 8896, \n",
    "                    8981, 9026, 9056, 9160, 9225, 9278, 9306, 9364, 9394, 9468, 9711, 9820, \n",
    "                    9834, 9859, 9936, 9981, 2403, 1904]) \n",
    "\n",
    "# Profiling\n",
    "start_time = time.time()\n",
    "\n",
    "# Auto-iterate through all files in the root folder.\n",
    "file_list = [file for file in os.listdir('.') if os.path.isfile(file)]\n",
    "print('Files on disk: {}'.format(len(file_list)))\n",
    "   \n",
    "# Main cycle\n",
    "for cur_seed in seeds[0:boots]:\n",
    "  filename = 'fsae_von_90_curseed_{}.pkl'.format(cur_seed)\n",
    "  matching_file = [ match for match in file_list if match == filename ]  # Get already simulated seeds\n",
    "  if len(matching_file) > 0:\n",
    "    # Seed already simulated. Skip to next cycle.\n",
    "    print('[%s]Seed %d ready!'% (time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),cur_seed)) \n",
    "    continue\n",
    "  \n",
    "  np.random.seed(cur_seed)\n",
    "  random.seed(cur_seed)\n",
    "  \n",
    "  # Loading train and test sets\n",
    "  print(\"[{}]Loading data... \".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "  sample = load_data()\n",
    "  \n",
    "  # Train individual models and SLR\n",
    "  print(\"[{}]Training models...\".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "  models_bundle = model_train(cur_seed, sample)\n",
    "  \n",
    "  # Calculate and print performance measures\n",
    "  print(\"[{}]Calculating performance measures...\".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "  evaluation = evaluate(sample, models_bundle)\n",
    "  \n",
    "  print('[%s]Seed %d ready!'% (time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),cur_seed), end='')\n",
    "  print('%d of %d total seeds.' % (seeds.tolist().index(cur_seed)+1, len(seeds)))\n",
    "  \n",
    "  data = {'seed':cur_seed, 'sample':sample, 'models_bundle':models_bundle, 'evaluation':evaluation }\n",
    "  \n",
    "  # Dumping state to disk...\n",
    "  print('[{}]Dumping to local disk file: {} ...'.format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),filename))\n",
    "  with open(filename, 'wb') as f:  \n",
    "    pickle.dump([data], f)\n",
    "  \n",
    "  print('DONE!')\n",
    "  continue\n",
    "\n",
    "print(\"DONE!\")\n",
    "print (\"All seeds evaluated!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnzQIkvEVfSx"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T12:53:27.721903Z",
     "start_time": "2019-06-13T12:53:02.650478Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "079N15oOVfS2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization of results\n",
    "\"\"\"\n",
    "# Reading available files on disk\n",
    "file_list = [file for file in os.listdir('.') if os.path.isfile(file)]\n",
    "print('Files on disk: {}'.format(len(file_list)))\n",
    "\n",
    "# PICKLE files\n",
    "file_pattern = 'fsae_von_90_'\n",
    "\n",
    "file_targets = [ match for match in file_list if match.startswith(file_pattern) ]  # Get already simulated seeds\n",
    "print('Target files: {}'.format(len(file_targets)))\n",
    "unified_data = pd.DataFrame()\n",
    "unified_coef = []\n",
    "\n",
    "win_freq = []\n",
    "names = ['SVR-RBF','LR','MLP','GP','Committee','SLR']\n",
    "for file in file_targets: \n",
    "  with open(file,'rb') as f:\n",
    "    data = pickle.load(f)[0]\n",
    "  data['evaluation'] = evaluate(data['sample'],data['models_bundle'])  \n",
    "  evaluation = data['evaluation']\n",
    "  unified_data = unified_data.append(data['evaluation']['indicators']) \n",
    "  unified_coef.append(data['evaluation']['coefficients'])\n",
    "  unified_data = unified_data.append(data['evaluation']['indicators']) \n",
    "  # Performance measures unification\n",
    "  # Coefficient of determination\n",
    "  r2s = [ r2_score(data['sample']['y'], cur.predict(data['sample']['X'])) for cur in data['models_bundle']['models'] ]\n",
    "  r2s[4:4] = [r2_score(data['sample']['y'], np.average(list(map(lambda x: x.predict(data['sample']['X']), data['models_bundle']['models'][0:-1])), axis=0))]\n",
    "  # Mean squared error\n",
    "  mses = [ mean_squared_error(data['sample']['ytest'], cur.predict(data['sample']['Xtest'])) for cur in data['models_bundle']['models'] ]\n",
    "  mses[4:4] = [mean_squared_error(data['sample']['ytest'], np.average(list(map(lambda x: x.predict(data['sample']['Xtest']), data['models_bundle']['models'][0:-1])), axis=0))]\n",
    "  # Median absolute error\n",
    "  maes = [ median_absolute_error(data['sample']['ytest'], cur.predict(data['sample']['Xtest'])) for cur in data['models_bundle']['models'] ]\n",
    "  maes[4:4] = [median_absolute_error(data['sample']['ytest'], np.average(list(map(lambda x: x.predict(data['sample']['Xtest']), data['models_bundle']['models'][0:-1])), axis=0))]\n",
    "  # Winner model (smaller MSE) identification\n",
    "  win_idx = np.where(mses == np.amin(mses))\n",
    "  win_freq.append(names[win_idx[0][0]])\n",
    "  # Specific seed visualization\n",
    "  if data['seed'] == 1024:\n",
    "    print('Weights for seed {}: {}'.format(data['seed'], evaluation['coefficients']))    \n",
    "    print(evaluation['indicators'])\n",
    "        \n",
    "# Set (te | tr)\n",
    "theSet = \"te\"\n",
    "# The indicator (MSE | MAE)\n",
    "theIndic = \"MSE\"\n",
    "# Indicator limit (sharing y-scales in continuous boxplots)\n",
    "# maxY = 20    # MAE Testing\n",
    "maxY = 2500  # MSE Testing\n",
    "# maxY = 700   # MSE Training\n",
    "\n",
    "toPlot = theIndic+theSet\n",
    "print('To plot: {}'.format(toPlot))\n",
    "\n",
    "tmp_palette = sns.color_palette(n_colors=5)\n",
    "tmp_palette.append((0.3, 0.6, 0.9))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10), gridspec_kw={'width_ratios': [4, 1, 1]})#, 'height_ratios':[3, 1.5]})\n",
    "sns.boxplot(data=unified_data.query('Method in [\"SVR-RBF\",\"LR\",\"MLP\",\"GP\"]'), x='Method', y=toPlot, notch=True, ax=axes[0][0], width=0.5, palette=tmp_palette)\n",
    "axes[0][0].set_ylabel('MSE')\n",
    "axes[0][0].set_xlabel(' ')\n",
    "axes[0][0].set_title('Individual surrogates')\n",
    "axes[0][0].set_ylim(0, maxY)#SLR-CV\n",
    "sns.boxplot(data=unified_data.query('Method == \"Committee\"'), x='Method', y=toPlot, notch=True, ax=axes[0][1], color=sns.color_palette(n_colors=5)[-1], width=0.5)\n",
    "axes[0][1].set_ylabel('MSE')\n",
    "axes[0][1].set_xlabel(' ')\n",
    "axes[0][1].set_title(\"Committee\")\n",
    "axes[0][1].set_ylim(0, maxY)#SLR-CV\n",
    "axes[0][1].axes.get_yaxis().set_ticklabels([])\n",
    "axes[0][1].axes.get_xaxis().set_ticklabels([])\n",
    "sns.boxplot(data=unified_data.query('Method == \"SLR\"'), x='Method', y=toPlot, notch=True, ax=axes[0][2], color=(0.3, 0.6, 0.9), width=0.5)\n",
    "axes[0][2].set_ylabel('MSE')\n",
    "axes[0][2].set_xlabel(' ')\n",
    "axes[0][2].set_title('SLR')\n",
    "axes[0][2].set_ylim(0, maxY)#SLR-CV\n",
    "axes[0][2].axes.get_yaxis().set_ticklabels([])\n",
    "axes[0][2].axes.get_xaxis().set_ticklabels([])\n",
    "\n",
    "sns.boxplot(x=\"variable\",y=\"value\",data=pd.melt(pd.DataFrame(unified_coef, columns=['SVR','LR ','MLP','GP'])), notch=True, width=0.4, ax=axes[1][0], palette=tmp_palette)\n",
    "axes[1][0].set_xlabel('Meta-regressor coefficients')\n",
    "axes[1][0].set_ylabel('Coefficient value')\n",
    "axes[1][0].axes.get_xaxis().set_ticklabels([])\n",
    "\n",
    "axes[1][0].set_xlabel('Meta-regressor coefficients')\n",
    "\n",
    "f, ax1 = plt.subplots(figsize=(5.3, 3))\n",
    "freqs = [ win_freq.count(X) for X in names]\n",
    "sns.barplot(data=pd.DataFrame({'models':names,'Frequency':freqs}), x='models', y='Frequency', palette=tmp_palette)\n",
    "\n",
    "print(\"\\nMSE\\tTrain\\tTest\\nCommittee\\t{:9.3f}\\t{:9.3f}\\nSLR\\t{:9.3f}\\t{:9.3f}\".format( \\\n",
    "   np.median(unified_data[unified_data.Method == 'Committee']['MSEtr']), \\\n",
    "   np.median(unified_data[unified_data.Method == 'Committee']['MSEte']), \\\n",
    "   np.median(unified_data[unified_data.Method == 'SLR']['MSEtr']), \\\n",
    "   np.median(unified_data[unified_data.Method == 'SLR']['MSEte'])))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Code for lecture - Stacked Linear Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "178px",
    "width": "316px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "508px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
